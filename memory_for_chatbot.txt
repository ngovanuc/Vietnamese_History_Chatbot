00:00:00	so in this video I'm going to be looking at memory and Lang chain so using memory in Lang chain if you want to jump ahead to the code I've put timestamp here and we'll show on the screen where you can actually just jump straight into the code so just quickly follow we look at why is memory important you know building chat agents and stuff like that one of the big reasons is that people treat any sort of chat bot or chat agent as basically like a human they kind of expect that it's going to have the

00:00:33	qualities of human and respond and they get very frustrated they don't realize that often these things are just doing one call and responding to that so you often find that things that people do is they want to abbreviate when they refer back to things that were spoken about earlier on in the conversation this can be talking about places times a whole bunch of different things and another big thing that you've need to factor in is the idea of co-reference resolution across the actual conversation so if someone starts

00:01:08	talking about a person and they actually give a name often they will later on just refer to that person as he or she and for the large language model to be able to pick up who that is we need to give it some kind of memory so that it can go back and work out okay who is this person that they're talking about or what is this topic that they're referring to that we've been having a conversation about so like I mentioned in the first video large language models themselves don't have a memory you're literally just pass

00:01:40	in prompt and then you get a conditional response based on the prompt that you pass in in the past people have tried to build Transformer models with some kind of memory hooked into them but nothing to date has been really optimal for this enough that people want to start using it at scale so this hopefully will come in the future is that we have some kind of built-in memory to the Transformer model and then large language models will be able to retain things internally at the moment we have two main options

00:02:13	for dealing with memory in a large language model we either one put it back into the prompt and the second way of doing this would be to have some kind of external lookup and this one we'll look at briefly in this video but in some of the future videos I'm going to look at a lot at using databases and using things for externally looking up information and then bringing that back into the prompt so the first version of doing this is basically where we just put the memory straight back into the prompt so

00:02:48	if we go back to our prompt that we talked about earlier on we've got a context so the context in this case is you're a digital assistant who enjoys having conversations called Kate and then you can see here we then just basically tell it right the current conversation that's gone on already and this would be sort of user you know one you know saying hi I am Sam the agent then says Hi Sam how are you I say I'm good what's your name the agent replies with my name is Kate and then the user saying how are you today Kate so you can

00:03:22	see all of this information would go into the prompt and this sort of raises some of the problems with having memory in these things is that while this is a good way to do it and it allows the language model to track what's going on in the conversation if we keep talking for an hour there's no way that that's gonna fit into the span of tokens that the language model can take the latest models from openai are using around 4 4096 tokens for the span while that is good that's great amount of information

00:03:59	that's not going to hold a full conversation so then this raises a whole bunch of issues of what are some of the ways that we can do this so Lang Gina has a whole bunch of these built in some of them doing just very simple things like what I showed you just before where we're just putting it into the prompt normally then we've got other ones that do things like summarize the conversation as it goes along we've got other ones where we limit the window so it only remembers the last few encounters and then we've got sort of

00:04:31	things where you know it merges some of those so where it has the last few but then it does a summary for everything else the summary though is actually done by calling out to a language model itself and asking it hey summarize this conversation so that's something that you will see as well then we've got some sort of more external ways of doing this so these can be things like making some kind of Knowledge Graph memory and putting things into sort of entity memory for doing these kinds of things

00:05:03	and then lastly you can just customize it right so you can come up if you've got a very unique case that you want to use the memory something yourself you can actually write your own system for doing memory with this in Lang chain as well let's jump into the code and look at how these things work Okay so we've got the normal sort of setup of installing open AI Lang chain Etc put your opening I key in there and the first one we're going to look at is basically the conversation buffer memory so in Lane chamber we need to import the

00:05:36	type of memory it is and then we will basically instantiate that and then we'll pass it into the chain so the chain that we're using here is just a simple conversation chain which allows us to basically converse with the model by doing composition predict and then passing in what we want to say and you'll see that as we go through this that I've got it set to verbose equals true so that we can see what the prompt is going in and we can then see the response that we're getting back so here we're basically setting up

00:06:07	the simplest kind of memory the conversation buffer and it's just going to basically track what the user says what the agent says and then just stack that into the prompt as we go through so you can see we start off with me saying hi there I'm Sam and we can see that the prompt that's going in this prompt here and we can see the current conversation basically is just human hi there I am Sam and then the AI is going to respond Hi Sam my name is AI it's nice to meet you what brings you here today

00:06:39	all right so then we're going to respond to that so in this case I'm going to ask it actually how are you today so I asked that so I didn't directly even answer it you know let's see how it handles it it's got no problem with that I'm doing great thanks for asking how about you then again I'm good thank you can you help me with some customer support so you notice that now we come into this one and it's stacking our conversation so this is the prompt that is going into the large language model and all this is

00:07:08	doing is just making this prompt longer each turn and I have asked it okay can you help me with customer support says absolutely what kind of customers what do you need and then I could keep going with this conversation but this is going to get longer and longer so at any point we can sort of look at the conversation memory buffer and we can see okay what actually was said so this allows us to if we want to save out the conversation we want to look at the conversation at any point we can just come in here and do this so you

00:07:37	can see here is the conversation exactly what we had up there above so that's the the simplest form of memory in Lang chain it's still very powerful especially if you've got something where you know that people are going to have a limited number of interactions with your Bot or you're actually going to sort of card code that after five interactions it shuts down you know something like that then this kind of memory will work really well the next kind of memory is conversation summary memory so here

00:08:10	we're basically again importing this in or instantiating the summary memory and we're going to put it in here so the difference now is rather than passing everything that I say and the bot says back and forth into the actual prompt it's now going to actually do a summary of that so you can see at the start we will get something like this okay hi hi there I'm Sam right it's just once Hi Sam my name is AI it's nice to meet you what brings you here today I said how are you today same as before but you

00:08:40	notice now on this second interaction so this is the second thing I've said so I've got first one up there second one I've said now and now it's not showing what I said the first time it's summarizing that so you can see the human introduces themselves as Sam and the AI introduces itself as AI the AI then asks what brings Sam here today so this is different now in this case we've actually used more tokens right at this stage but you'll see that as we go through if I can say okay can you help me with some customer support now when

00:09:16	we go to the next summary the human introduces themselves as Sam and she says is AI then finally gets to the AI is doing great and then then it will give the last thing so this is sort of at the start maybe it's actually adding more tokens because you're getting a sort of summarized version of the whole conversation but over time this is definitely going to be used less tokens so you'll see that here now it's answering and then if I went on with this conversation I I would then be able to see that okay

00:09:48	it's not actually storing what we say verbatim it's storing a summary of the conversation as a whole so here if we come down and look at conversation.memory.buffer we can see our summary and our summary is basically of the whole thing so each time that it does a new summary so we've got multiple calls to a large language model here we've got the call for the response but we've also got a call for actually summarizing the conversation so after the first step the first step doesn't need a summary but after the first step

00:10:21	you've got a summary of of the conversation at each point and we can basically just print that out and we can look at it and we can see that yeah response is doing great when sem asks for customer support the AI responds positively and asks what kind of Customs support Sam needs so you'll notice here that it's kind of doing a co-reference resolution in that rather than a human would probably say it responds to what he asked or she here it's sticking to Ai and Sam in there so this is a summary one it's also very

00:10:54	useful the next one is is kind of an alternate version of the first one that we had so this is a conversation buffer window memory so we're going to be doing the same sort of thing of just feeding the conversation into the prompt but the difference is now we're going to set the last number of interactions that we are sitting in here now I've set this very low just so we can see it here uh but you could actually set this to be much higher than this you could get like the last five or ten interactions depending

00:11:25	on how many tokens you want to use and this also comes down to also how much money you want to spend for this kind of thing so it's instantiating it setting k equals 2 here we've got a conversation hi there I'm Sam what brings you here today I'm looking for some customer support so it's asking me what kind of customer support you need my TV is not working so now we've got our two things going so now it's gonna ask me again sorry here for that it's asking for more information I give it the information

00:11:55	but you notice now when I tell it I turn you turn it on make some weird noises and then goes black and the prompt now has the last interactions right the last two full interactions that we've had plus this new one but it's lost that first one where I said hi I'm Sam so it's just feeding the last two interactions into the the large language model so if you've got something where like let's say we set it to k equals five most conversations probably aren't Gonna Change hugely it will sometimes refer back to things early on in the

00:12:34	conversation but a lot of things a lot of times you can sort of fool people with just a very short memory with a memory of just three to five steps back in the conversation you can see if we look at the conversation memory buffer though it's still got the full conversation in there so if we wanted to use that to to look at it or store it some the next one is kind of a combination of the first ones in the now we're gonna have a summary like the second one we looked at but we're also going to have a

00:13:06	buffer in there as well right so here we're basically sending it to be have a buffer of the number of tokens where we're going to have basically the tokens is going to be limited to 40 tokens so we could set it you know a few different ways for that but we've instantiated this this memory like that I and then we're going to go through it again so just quickly hi there I'm Sam nothing really different going on here we've got the I need help with my broken TV and you can see it's just taking in steps of

00:13:36	the conversation at this point and asked me why I basically tell it's what's wrong and suddenly now in this interaction we've now gone beyond where we were so now it's doing a summary of the early steps so the human Sam introduces themselves to the AI the AI responds and asks what brings Sam to them today and then it go then it gives us the action actual conversation for the last K number of steps or the last number of you know tokens in in this case so you can see here that okay I've entered that it doesn't sound good you

00:14:10	know if it's a hardware I say it seems to be a hardware issue and sure enough now the summary is updated as well right so we've lost one uh one step in the conversation but we've updated the summary to include that right the AI expresses sympathy and asks what the problem is in there so this is kind of like giving you a little bit of The Best of Both Worlds in there at any point too we can actually sort of print out the sort of moving summary buffer so if we want to see like okay what's the summary at this

00:14:41	point of the conversation so this could be useful for a variety of different things if you want to have some other module extract information from the conversation or something like that you can try passing this into that as it goes through I the next two a little bit different so the next two it's trying to basically represent the conversation and extract information from the conversation based on what's said in the form of sort of entities and Knowledge Graph memory so this one is the knowledge graph

00:15:16	memory so conversation kg kg is for Knowledge Graph rest of it's the same we've got a simple sort of prompt in there and I've just put the prompt in there just so you can sort of see you know what what it's it's actually doing is that it's it's added to the prompt now with the AI only uses information contained in relevant information section and does not hallucinate so the idea here is we're trying to get it more to just focus on what we said and not add to that in any way we instantiate it just like like normal

00:15:47	we've got our conversation game there I am Sam starts off like normal but you'll see that anytime that it thinks that there's relevant information it will pass that in so if I go through and basically say right you know it's you this is what's wrong with it and I give it you know some information it's actually taking that information and constructing a sort of little mini Knowledge Graph for that so it actually you should be able to sort of plot this out but just showing you this rather than plotting it out is that we see that

00:16:19	we've got this sort of little Network X graph that's been made so we can see that Sam so okay the first two are nodes and this the third one is always what's connecting them so okay Sam is human TV is broken True TV weird sounds makes TV black goes black TV is under warranty yes right device is under warranty and this is the the warranty number that we've got here so you can see it's extracted some really useful information for that and this could be really useful for things like if you wanted to fire

00:16:56	some other prompt or do something different once you know that it's about a TV this would be able to extract that out easily and pass that information to to your Bot which can then respond in in different ways or could then you know break off into different chains Etc entity memory so this is doing a similar kind of thing just in now that it's looking for specific kind of entities yeah and we could even put some examples of that into the prompt if we wanted to do this so have a read of the prompt the

00:17:27	prompt's kind of interesting how that's done I but then basically you've got the same sort of conversation chain we we're passing in the the this The Prompt we're passing in the instantiated conversational memory here we've got our same conversation I've abbreviated a little bit here hi I'm Sam my TV is broken but it's under warranty right so now we can see that it's found some entities there that's like oh okay it's got Sam it's got TV right and then it responds to that you know hi I'm sorry to hear that YouTube is breaking

00:18:00	is there anything I can do to help how can I get it fixed the warranty number is and I give it that so you can see it's extracted that out from this and it's it's got that in case it wants to use it going forward and then and sure enough it does use that right we can see that okay it looks like your TV is still under warranty to get it fixed you'll need to contact the manufacturer and provide them with the warranty number and it gives me the warranty number back which it's got in its context now

00:18:27	so if we look at the entity memory from the last step we can see that okay we've got that warranty that's what it got from the last step that's in its cache let's go on with this can you send the repair person called Dave to fix it and we can see that it's worked out that okay there's a there's an entity there of Dave says sure I can send Dave to fix your TV can you provide me with his contact information so it's not kind of clued in that it's supposed to be the TV repair place but anyway finally so we've

00:18:57	done quite a number of steps now finally we can look at the you know the last entity that came out was Dave but we can also just print out the entire memory store here that's got all the entities that it's got and you see sure enough that it's it's got this and then it's got okay this is the warranty number for Sam's TV and notice that I never said that right it's rephrased what I've said to do that uh it's got Dave Dave is a repair person it's got Sam Sam owns a TV that is currently broken and under

00:19:26	warranty and then we've got TV the TV is under warranty so entity memory can be really useful for extracting things out if people are talking about relationships or something like that this can be really good if someone starts telling a story to the bot and they're telling you know oh and my sister is called this and and her son is this you know this is one of the ways you could use to extract that kind of thing so I haven't got an example of the custom memory one here if people are interested let me know in the comments

00:19:55	and I'll I'll build one to show you but you can sort of combine different memories as well on top of this perhaps we'll look at that maybe later on but this gives you a sense of like what the memories are and how they get used in these Bots for building your own conversational agent all right if this was helpful to you please give it a like And subscribe I've got more videos of Lang chain coming talk to you soon

