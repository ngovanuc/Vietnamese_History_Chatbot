{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23cd9b9",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3fc89970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "11ea4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c1227db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUỔI ĐẦU LỊCH SỬ NƯỚC TA</td>\n",
       "      <td>Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA</td>\n",
       "      <td>chapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA</td>\n",
       "      <td>Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA\\nBài: THỜI NG...</td>\n",
       "      <td>lesson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Những dấu tích của Người tối cổ được tìm thấy ...</td>\n",
       "      <td>Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nNhững ...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ở giai đoạn đầu, Người tinh khôn sống như thế ...</td>\n",
       "      <td>Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nỞ giai...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giai đoạn phát triển của Người tinh khôn có gì...</td>\n",
       "      <td>Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nGiai đ...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                           BUỔI ĐẦU LỊCH SỬ NƯỚC TA   \n",
       "1                  THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA   \n",
       "2  Những dấu tích của Người tối cổ được tìm thấy ...   \n",
       "3  Ở giai đoạn đầu, Người tinh khôn sống như thế ...   \n",
       "4  Giai đoạn phát triển của Người tinh khôn có gì...   \n",
       "\n",
       "                                             content     type  \n",
       "0                   Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA  chapter  \n",
       "1  Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA\\nBài: THỜI NG...   lesson  \n",
       "2  Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nNhững ...    title  \n",
       "3  Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nỞ giai...    title  \n",
       "4  Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nGiai đ...    title  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vietnamese_history_dataset_path = \"../../datasets/vietnamese_history_dataset/data.json\"\n",
    "df = pd.read_json(vietnamese_history_dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1a4d3dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "      <th>content_embedding</th>\n",
       "      <th>content_embeded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUỔI ĐẦU LỊCH SỬ NƯỚC TA</td>\n",
       "      <td>Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA</td>\n",
       "      <td>chapter</td>\n",
       "      <td>BUỔI ĐẦU LỊCH SỬ NƯỚC TA Chương: BUỔI ĐẦU LỊCH...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA</td>\n",
       "      <td>Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA\\nBài: THỜI NG...</td>\n",
       "      <td>lesson</td>\n",
       "      <td>THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA Chương: BUỔI...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Những dấu tích của Người tối cổ được tìm thấy ...</td>\n",
       "      <td>Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nNhững ...</td>\n",
       "      <td>title</td>\n",
       "      <td>Những dấu tích của Người tối cổ được tìm thấy ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ở giai đoạn đầu, Người tinh khôn sống như thế ...</td>\n",
       "      <td>Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nỞ giai...</td>\n",
       "      <td>title</td>\n",
       "      <td>Ở giai đoạn đầu, Người tinh khôn sống như thế ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giai đoạn phát triển của Người tinh khôn có gì...</td>\n",
       "      <td>Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nGiai đ...</td>\n",
       "      <td>title</td>\n",
       "      <td>Giai đoạn phát triển của Người tinh khôn có gì...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                           BUỔI ĐẦU LỊCH SỬ NƯỚC TA   \n",
       "1                  THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA   \n",
       "2  Những dấu tích của Người tối cổ được tìm thấy ...   \n",
       "3  Ở giai đoạn đầu, Người tinh khôn sống như thế ...   \n",
       "4  Giai đoạn phát triển của Người tinh khôn có gì...   \n",
       "\n",
       "                                             content     type  \\\n",
       "0                   Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA  chapter   \n",
       "1  Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA\\nBài: THỜI NG...   lesson   \n",
       "2  Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nNhững ...    title   \n",
       "3  Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nỞ giai...    title   \n",
       "4  Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nGiai đ...    title   \n",
       "\n",
       "                                   content_embedding content_embeded  \n",
       "0  BUỔI ĐẦU LỊCH SỬ NƯỚC TA Chương: BUỔI ĐẦU LỊCH...            None  \n",
       "1  THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA Chương: BUỔI...            None  \n",
       "2  Những dấu tích của Người tối cổ được tìm thấy ...            None  \n",
       "3  Ở giai đoạn đầu, Người tinh khôn sống như thế ...            None  \n",
       "4  Giai đoạn phát triển của Người tinh khôn có gì...            None  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"content_embedding\"] = None\n",
    "df[\"content_embeded\"] = None\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    df[\"content_embedding\"][idx] = df[\"title\"][idx] + \" \" + df[\"content\"][idx]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2849bbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../embedding_models\\\\all-MiniLM-L6-v2',\n",
       " '../../embedding_models\\\\bert-base-vietnamese-uncased',\n",
       " '../../embedding_models\\\\distiluse-base-multilingual-cased-v2',\n",
       " '../../embedding_models\\\\multilingual-e5-small',\n",
       " '../../embedding_models\\\\phobert-base-v2',\n",
       " '../../embedding_models\\\\phobert-large',\n",
       " '../../embedding_models\\\\README.md',\n",
       " '../../embedding_models\\\\sup-SimCSE-VietNamese-phobert-base',\n",
       " '../../embedding_models\\\\vietnamese-bi-encoder',\n",
       " '../../embedding_models\\\\vietnamese-embedding']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "glob.glob(\"../../embedding_models/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c3e30921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA ❉ ♡ƪ(ˆ◡ˆ)ʃ♪ 卍 ₯\n"
     ]
    }
   ],
   "source": [
    "special_characters = \"❉ ♡ƪ(ˆ◡ˆ)ʃ♪ 卍 ₯\"\n",
    "sentence = df[\"content\"][0] + \" \" + special_characters\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357b726",
   "metadata": {},
   "source": [
    "# Các chỉ số đánh giá hiệu quả tokenizer của các model embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ef649",
   "metadata": {},
   "source": [
    "| Chỉ số                            | Mô tả                                                                                                     | Cách tính                                                                                                                                                                                  |\n",
    "| --------------------------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **1. avg\\_tokens\\_per\\_sample**   | Số **token trung bình** trên mỗi đoạn văn bản (sample)                                                    | Tính trung bình số token: <br> $\\text{Avg} = \\frac{1}{N} \\sum_{i=1}^{N} T_i$ <br> với $T_i$ là số token của văn bản thứ i                                                                  |\n",
    "| **2. std\\_tokens\\_per\\_sample**   | **Độ lệch chuẩn** số token giữa các văn bản                                                               | Dùng công thức độ lệch chuẩn: <br> $\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} (T_i - \\text{Avg})^2 }$                                                                                     |\n",
    "| **3. max\\_tokens**                | Số lượng **token lớn nhất** trong 1 đoạn văn bản                                                          | $\\max(T_1, T_2, ..., T_N)$                                                                                                                                                                 |\n",
    "| **4. min\\_tokens**                | Số lượng **token nhỏ nhất** trong 1 đoạn văn bản                                                          | $\\min(T_1, T_2, ..., T_N)$                                                                                                                                                                 |\n",
    "| **5. subword\\_rate**              | **Tỷ lệ token là subword** (từ bị tách ra thành các mảnh nhỏ) <br> ví dụ “học\\_sinh” → \\[\"học\", \"\\_sinh\"] | $\\text{Subword Rate} = \\frac{\\text{Số token có dạng subword}}{\\text{Tổng số token}}$ <br> Cách nhận dạng subword tùy tokenizer: <br> - `##` (BERT) <br> - không có `▁` đầu (SentencePiece) |\n",
    "| **6. out\\_of\\_vocab\\_rate (OOV)** | **Tỷ lệ token bị thay thế bằng mã không xác định** như `[UNK]`, `<unk>`                                   | $\\text{OOV Rate} = \\frac{\\text{Số token là [UNK] hoặc <unk>}}{\\text{Tổng số token}}$                                                                                                       |\n",
    "| **7. time\\_per\\_1k\\_samples**     | **Thời gian thực thi tokenization cho 1000 văn bản**                                                      | Đo bằng `time.time()` trước và sau khi tokenizer chạy trên 1000 mẫu: <br> $\\text{Time} = \\text{end\\_time} - \\text{start\\_time}$                                                            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fd24c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_tokens_length(dataframe, tokenizer):\n",
    "    \"\"\"\n",
    "    Tính toán trung bình số lượng tokens có trong một sample trên toàn bộ dataset.\n",
    "\n",
    "    Args:\n",
    "        - dataframe: Một dataframe có cột tên là \"content_embedding\", cột này chứa văn bản cần embedding.\n",
    "        - tokenizer: Một đối tượng tokenizer (AutoTokenizer) dùng để tokenization văn bản.\n",
    "    \n",
    "    Returns:\n",
    "        - list_tokens: Một 2D array, là toàn bộ tokens của dataset. Ex: [[tokens of a sample], [tokens of another sample]].\n",
    "        - average_tokens: Trung bình số lượng tokens có trong một sample trên toàn bộ dataset\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    list_tokens = []\n",
    "    for idx in range(len(dataframe)):\n",
    "        sentence = dataframe[\"content_embedding\"][idx]\n",
    "        tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        list_tokens.append(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))\n",
    "        total_tokens += len(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))\n",
    "    average_tokens = total_tokens / len(dataframe)\n",
    "    return list_tokens, average_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c0d72ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_tokens_length(list_tokens, average_tokens):\n",
    "    \"\"\"\n",
    "    Tính toán độ lệch chuẩn số token giữa các sample trên toàn bộ dataset.\n",
    "\n",
    "    Args:\n",
    "        - list_tokens: 2D array, là toàn bộ tokens của dataset. Ex: [[tokens of a sample], [tokens of another sample]].\n",
    "        - average_tokens: Trung bình lượng tokens trong một sample trên toàn bộ dataset.\n",
    "\n",
    "    Returns:\n",
    "        - Độ lệch chuẩn\n",
    "    \"\"\"\n",
    "    M = 0\n",
    "    for tokens in list_tokens:\n",
    "        M += (len(tokens) - average_tokens) ** 2\n",
    "    return (M / len(list_tokens)) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "715d4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_token_length(list_tokens):\n",
    "    \"\"\"\n",
    "    Tìm sample có số lượng tokens nhỏ nhất và lớn nhất.\n",
    "    \n",
    "    Args:\n",
    "        - list_tokens: 2D array, là toàn bộ tokens của dataset. Ex: [[tokens of a sample], [tokens of another sample]]\n",
    "\n",
    "    Returns:\n",
    "        - Lượng tokenss ít nhất trong một sample.\n",
    "        - Lượng tokens nhiều nhất trong một sample.\n",
    "    \"\"\"\n",
    "    min_tokens = 0\n",
    "    max_tokens = 0\n",
    "    for tokens in list_tokens:\n",
    "        if len(tokens) < min_tokens:\n",
    "            min_tokens = len(tokens)\n",
    "        if len(tokens) > max_tokens:\n",
    "            max_tokens = len(tokens)\n",
    "\n",
    "    return min_tokens, max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1d881576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword_rate(list_tokens, subword_character):\n",
    "    \"\"\"\n",
    "    Tính toán tỉ lệ token là subword trên toàn bộ dataset.\n",
    "    \n",
    "    Arg:\n",
    "        - list_tokens: 2D array, là toàn bộ tokens của dataset. Ex: [[tokens of a sentences], [tokens of another sentences]]\n",
    "        - subword_character: Một ký tự đánh dấu token đó là subword. Ex: ##, _, @@\n",
    "\n",
    "    Returns:\n",
    "        - Tỉ lệ tokens bị phân tách thành subword.\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    num_subword_tokens = 0\n",
    "    for tokens in list_tokens:\n",
    "        for token in tokens:\n",
    "            if subword_character in token:\n",
    "                num_subword_tokens += 1\n",
    "        total_tokens += len(tokens)\n",
    "    return num_subword_tokens / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ff0b38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_vocab_rate(list_tokens, oov_character):\n",
    "    \"\"\"\n",
    "    Tỉ lệ tokens bị thay thế bằng mã không xác định <UNK>/ <unk> (out-of-vocab rate) trên toàn bộ đataset.\n",
    "\n",
    "    Args:\n",
    "        - list_tokens: Một 2D array, là toàn bộ tokens của dataset. Ex: [[tokens of a sentences], [tokens of another sentences]]\n",
    "        - oov_character (out-of-vocab): Ký tự giúp nhận viết đó là một token oov.\n",
    "    \"\"\"\n",
    "    num_oov = 0\n",
    "    total_tokens = 0\n",
    "    for tokens in list_tokens:\n",
    "        for token in tokens:\n",
    "            if token == oov_character:\n",
    "                num_oov += 1\n",
    "        total_tokens += len(tokens)\n",
    "\n",
    "    return num_oov / total_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4d73d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_categories(dataframe, tokenizer, subword_character, oov_character):\n",
    "    \"\"\"\n",
    "    Gọi tất cả các hàm trên.\n",
    "    \"\"\"\n",
    "    list_tokens, average_tokens = average_tokens_length(dataframe=dataframe, tokenizer=tokenizer)\n",
    "    print(f\"Average tokens length: {average_tokens}\")\n",
    "\n",
    "    std_tokens_ = std_tokens_length(list_tokens=list_tokens, average_tokens=average_tokens)\n",
    "    print(f\"Std of tokens length: {std_tokens_}\")\n",
    "\n",
    "    min_tokens, max_tokens = min_max_token_length(list_tokens=list_tokens)\n",
    "    print(f\"Minimum of tokens on a sample: {min_tokens}\")\n",
    "    print(f\"Maximum of tokens on a sample: {max_tokens}\")\n",
    "\n",
    "    subword_rate_ = subword_rate(list_tokens=list_tokens, subword_character=subword_character)\n",
    "    print(f\"Subword rate: {subword_rate_}\")\n",
    "\n",
    "    oov_rate = out_of_vocab_rate(list_tokens=list_tokens,oov_character=oov_character)\n",
    "    print(f\"Out-of-vocab rate: {oov_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447c238",
   "metadata": {},
   "source": [
    "# all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2624428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "path = \"../../embedding_models/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "98f5aecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "3\n",
      "{'input_ids': tensor([[  101, 14684,  5063,  1024, 20934, 10448,  1102,  4887,  5622,  2818,\n",
      "         10514, 16371, 10085, 11937,   100,   100,  1006,   100,  1007,   100,\n",
      "           100,   100,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# Tokenizer cho câu đầu vào\n",
    "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "print(len(inputs))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8ed95bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 23])\n",
      "tensor([[  101, 14684,  5063,  1024, 20934, 10448,  1102,  4887,  5622,  2818,\n",
      "         10514, 16371, 10085, 11937,   100,   100,  1006,   100,  1007,   100,\n",
      "           100,   100,   102]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 23])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 23])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "==========================\n",
      "['[CLS]', 'chu', '##ong', ':', 'bu', '##oi', 'đ', '##au', 'li', '##ch', 'su', 'nu', '##oc', 'ta', '[UNK]', '[UNK]', '(', '[UNK]', ')', '[UNK]', '[UNK]', '[UNK]', '[SEP]']\n",
      "==========================\n",
      "chu -> 14684\n",
      "##ong -> 5063\n",
      ": -> 1024\n",
      "bu -> 20934\n",
      "##oi -> 10448\n",
      "đ -> 1102\n",
      "##au -> 4887\n",
      "li -> 5622\n",
      "##ch -> 2818\n",
      "su -> 10514\n",
      "nu -> 16371\n",
      "##oc -> 10085\n",
      "ta -> 11937\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "( -> 1006\n",
      "[UNK] -> 100\n",
      ") -> 1007\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n"
     ]
    }
   ],
   "source": [
    "# Show inputs\n",
    "for k, v in inputs.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()\n",
    "\n",
    "# Hiển thị chunking\n",
    "print(\"==========================\")\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "\n",
    "# Hiển thị token và mã tương ứng\n",
    "print(\"==========================\")\n",
    "for token, token_id in zip(tokenizer.tokenize(sentence), tokenizer(sentence)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a66e262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens length: 332.71097234611955\n",
      "Std of tokens length: 195.5535530481586\n",
      "Minimum of tokens on a sample: 0\n",
      "Maximum of tokens on a sample: 512\n",
      "Subword rate: 0.35267810461459265\n",
      "Out-of-vocab rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "all_categories(dataframe=df, tokenizer=tokenizer, subword_character=\"##\", oov_character=\"[UNK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5186dea",
   "metadata": {},
   "source": [
    "# bert-base-vietnamese-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1e5cfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "path = \"../../embedding_models/bert-base-vietnamese-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "49d80645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000000000019884624838656\n",
      "3\n",
      "{'input_ids': tensor([[    2, 22360,     1,  3710,   374,    27,   881, 15780,  1460,  3086,\n",
      "          3332,   389,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# Tokenizer cho câu đầu vào\n",
    "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "print(len(inputs))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f8b2642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 21])\n",
      "tensor([[    2, 22360,     1,  3710,   374,    27,   881, 15780,  1460,  3086,\n",
      "          3332,   389,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             3]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 21])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 21])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "==========================\n",
      "['[CLS]', 'chuong', '[UNK]', 'bu', '##o', '##i', 'đau', 'lich', 'su', 'nu', '##oc', 'ta', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[SEP]']\n",
      "==========================\n",
      "chuong -> 22360\n",
      "[UNK] -> 1\n",
      "bu -> 3710\n",
      "##o -> 374\n",
      "##i -> 27\n",
      "đau -> 881\n",
      "lich -> 15780\n",
      "su -> 1460\n",
      "nu -> 3086\n",
      "##oc -> 3332\n",
      "ta -> 389\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n"
     ]
    }
   ],
   "source": [
    "# Show inputs\n",
    "for k, v in inputs.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()\n",
    "\n",
    "# Hiển thị chunking\n",
    "print(\"==========================\")\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "\n",
    "# Hiển thị token và mã tương ứng\n",
    "print(\"==========================\")\n",
    "for token, token_id in zip(tokenizer.tokenize(sentence), tokenizer(sentence)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e3034f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens length: 402.99107939339876\n",
      "Std of tokens length: 354.1514993442404\n",
      "Minimum of tokens on a sample: 0\n",
      "Maximum of tokens on a sample: 2240\n",
      "Subword rate: 0.16576979012867651\n",
      "Out-of-vocab rate: 0.11234900487655865\n"
     ]
    }
   ],
   "source": [
    "all_categories(dataframe=df, tokenizer=tokenizer, subword_character=\"##\", oov_character=\"[UNK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5787ae77",
   "metadata": {},
   "source": [
    "# distiluse-base-multilingual-cased-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e0b5e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "path = \"../../embedding_models/distiluse-base-multilingual-cased-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e6d69adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "2\n",
      "{'input_ids': tensor([[   101,  88797,    131,    100,    100,    100,    100,    100,  91075,\n",
      "            100,    100,    113,    100,    114,    400, 111756,   2673,    100,\n",
      "            102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# Tokenizer cho câu đầu vào\n",
    "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "print(len(inputs))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9173de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 19])\n",
      "tensor([[   101,  88797,    131,    100,    100,    100,    100,    100,  91075,\n",
      "            100,    100,    113,    100,    114,    400, 111756,   2673,    100,\n",
      "            102]])\n",
      "\n",
      "attention_mask: torch.Size([1, 19])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "==========================\n",
      "['[CLS]', 'Chương', ':', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'TA', '[UNK]', '[UNK]', '(', '[UNK]', ')', 'ʃ', '##♪', '卍', '[UNK]', '[SEP]']\n",
      "==========================\n",
      "Chương -> 88797\n",
      ": -> 131\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "TA -> 91075\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "( -> 113\n",
      "[UNK] -> 100\n",
      ") -> 114\n",
      "ʃ -> 400\n",
      "##♪ -> 111756\n",
      "卍 -> 2673\n",
      "[UNK] -> 100\n"
     ]
    }
   ],
   "source": [
    "# Show inputs\n",
    "for k, v in inputs.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()\n",
    "\n",
    "# Hiển thị chunking\n",
    "print(\"==========================\")\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "\n",
    "# Hiển thị token và mã tương ứng\n",
    "print(\"==========================\")\n",
    "for token, token_id in zip(tokenizer.tokenize(sentence), tokenizer(sentence)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6ea6d8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens length: 295.4228367528992\n",
      "Std of tokens length: 188.4914893252836\n",
      "Minimum of tokens on a sample: 0\n",
      "Maximum of tokens on a sample: 512\n",
      "Subword rate: 0.11856786112226687\n",
      "Out-of-vocab rate: 0.016734658135272324\n"
     ]
    }
   ],
   "source": [
    "all_categories(dataframe=df, tokenizer=tokenizer, subword_character=\"##\", oov_character=\"[UNK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8d562",
   "metadata": {},
   "source": [
    "# multilingual-e5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "dd739403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "path = \"../../embedding_models/multilingual-e5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8182a3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "2\n",
      "{'input_ids': tensor([[     0,  40691,     12,  22013,  99973,    568,   4428, 240532,   1062,\n",
      "            339, 119393,  16999,    159, 160527,    541, 227860,  12998,      6,\n",
      "         247723,      6,  61893,      3,    132, 244806, 245728, 244806,     16,\n",
      "         245313,  26547,      6, 246342,      6,      3,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# Tokenizer cho câu đầu vào\n",
    "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "print(len(inputs))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "22440452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 34])\n",
      "tensor([[     0,  40691,     12,  22013,  99973,    568,   4428, 240532,   1062,\n",
      "            339, 119393,  16999,    159, 160527,    541, 227860,  12998,      6,\n",
      "         247723,      6,  61893,      3,    132, 244806, 245728, 244806,     16,\n",
      "         245313,  26547,      6, 246342,      6,      3,      2]])\n",
      "\n",
      "attention_mask: torch.Size([1, 34])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "==========================\n",
      "['<s>', '▁Chương', ':', '▁BU', 'Ổ', 'I', '▁Đ', 'Ầ', 'U', '▁L', 'Ị', 'CH', '▁S', 'Ử', '▁N', 'ƯỚC', '▁TA', '▁', '❉', '▁', '♡', '<unk>', '(', 'ˆ', '◡', 'ˆ', ')', 'ʃ', '♪', '▁', '卍', '▁', '<unk>', '</s>']\n",
      "==========================\n",
      "▁Chương -> 40691\n",
      ": -> 12\n",
      "▁BU -> 22013\n",
      "Ổ -> 99973\n",
      "I -> 568\n",
      "▁Đ -> 4428\n",
      "Ầ -> 240532\n",
      "U -> 1062\n",
      "▁L -> 339\n",
      "Ị -> 119393\n",
      "CH -> 16999\n",
      "▁S -> 159\n",
      "Ử -> 160527\n",
      "▁N -> 541\n",
      "ƯỚC -> 227860\n",
      "▁TA -> 12998\n",
      "▁ -> 6\n",
      "❉ -> 247723\n",
      "▁ -> 6\n",
      "♡ -> 61893\n",
      "ƪ -> 3\n",
      "( -> 132\n",
      "ˆ -> 244806\n",
      "◡ -> 245728\n",
      "ˆ -> 244806\n",
      ") -> 16\n",
      "ʃ -> 245313\n",
      "♪ -> 26547\n",
      "▁ -> 6\n",
      "卍 -> 246342\n",
      "▁ -> 6\n",
      "₯ -> 3\n"
     ]
    }
   ],
   "source": [
    "# Show inputs\n",
    "for k, v in inputs.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()\n",
    "\n",
    "# Hiển thị chunking\n",
    "print(\"==========================\")\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "\n",
    "# Hiển thị token và mã tương ứng\n",
    "print(\"==========================\")\n",
    "for token, token_id in zip(tokenizer.tokenize(sentence), tokenizer(sentence)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens length: 293.45673505798396\n",
      "Std of tokens length: 185.51125237072657\n",
      "Minimum of tokens on a sample: 0\n",
      "Maximum of tokens on a sample: 512\n",
      "Subword rate: 0.77908288115757\n",
      "Out-of-vocab rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "all_categories(dataframe=df, tokenizer=tokenizer, subword_character=\"▁\", oov_character=\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493abeed",
   "metadata": {},
   "source": [
    "# phobert-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f9804dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../../embedding_models/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "path = \"../../embedding_models/phobert-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "94a99da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000000000019884624838656\n",
      "3\n",
      "{'input_ids': tensor([[    0,  1735, 50410,    27,   924,  1878, 59831,  1782, 28661,  3521,\n",
      "          1042, 31584,  6506,   870, 55132,  1086, 55296, 10875,     3,     3,\n",
      "             3, 14157,     3,     3,     3, 37272, 63120,     3,     3,     3,\n",
      "             2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# Tokenizer cho câu đầu vào\n",
    "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "print(len(inputs))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4490f493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 31])\n",
      "tensor([[    0,  1735, 50410,    27,   924,  1878, 59831,  1782, 28661,  3521,\n",
      "          1042, 31584,  6506,   870, 55132,  1086, 55296, 10875,     3,     3,\n",
      "             3, 14157,     3,     3,     3, 37272, 63120,     3,     3,     3,\n",
      "             2]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 31])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 31])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "==========================\n",
      "['<s>', 'Ch@@', 'ương@@', ':', 'B@@', 'U@@', 'ỔI', 'Đ@@', 'Ầ@@', 'U', 'L@@', 'Ị@@', 'CH', 'S@@', 'Ử', 'N@@', 'ƯỚC', 'TA', '<unk>', '<unk>', '<unk>', '(@@', '<unk>', '<unk>', '<unk>', ')@@', 'ʃ@@', '<unk>', '<unk>', '<unk>', '</s>']\n",
      "==========================\n",
      "Ch@@ -> 1735\n",
      "ương@@ -> 50410\n",
      ": -> 27\n",
      "B@@ -> 924\n",
      "U@@ -> 1878\n",
      "ỔI -> 59831\n",
      "Đ@@ -> 1782\n",
      "Ầ@@ -> 28661\n",
      "U -> 3521\n",
      "L@@ -> 1042\n",
      "Ị@@ -> 31584\n",
      "CH -> 6506\n",
      "S@@ -> 870\n",
      "Ử -> 55132\n",
      "N@@ -> 1086\n",
      "ƯỚC -> 55296\n",
      "TA -> 10875\n",
      "❉ -> 3\n",
      "♡@@ -> 3\n",
      "ƪ@@ -> 3\n",
      "(@@ -> 14157\n",
      "ˆ@@ -> 3\n",
      "◡@@ -> 3\n",
      "ˆ@@ -> 3\n",
      ")@@ -> 37272\n",
      "ʃ@@ -> 63120\n",
      "♪ -> 3\n",
      "卍 -> 3\n",
      "₯ -> 3\n"
     ]
    }
   ],
   "source": [
    "# Show inputs\n",
    "for k, v in inputs.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()\n",
    "\n",
    "# Hiển thị chunking\n",
    "print(\"==========================\")\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "\n",
    "# Hiển thị token và mã tương ứng\n",
    "print(\"==========================\")\n",
    "for token, token_id in zip(tokenizer.tokenize(sentence), tokenizer(sentence)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ad5ddd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens length: 368.921498661909\n",
      "Std of tokens length: 313.0481961962608\n",
      "Minimum of tokens on a sample: 0\n",
      "Maximum of tokens on a sample: 1932\n",
      "Subword rate: 0.2056794523661564\n",
      "Out-of-vocab rate: 0.008985373379017848\n"
     ]
    }
   ],
   "source": [
    "all_categories(dataframe=df, tokenizer=tokenizer, subword_character=\"@@\", oov_character=\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a187a8",
   "metadata": {},
   "source": [
    "# phobert-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a1105eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "path = \"../../embedding_models/phobert-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2af3fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000000000019884624838656\n",
      "3\n",
      "{'input_ids': tensor([[    0,  1735, 50410,    27,   924,  1878, 59831,  1782, 28661,  3521,\n",
      "          1042, 31584,  6506,   870, 55132,  1086, 55296, 10875,     3,     3,\n",
      "             3, 14157,     3,     3,     3, 37272, 63120,     3,     3,     3,\n",
      "             2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# Tokenizer cho câu đầu vào\n",
    "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "print(len(inputs))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c57b2b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 31])\n",
      "tensor([[    0,  1735, 50410,    27,   924,  1878, 59831,  1782, 28661,  3521,\n",
      "          1042, 31584,  6506,   870, 55132,  1086, 55296, 10875,     3,     3,\n",
      "             3, 14157,     3,     3,     3, 37272, 63120,     3,     3,     3,\n",
      "             2]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 31])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 31])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "==========================\n",
      "['<s>', 'Ch@@', 'ương@@', ':', 'B@@', 'U@@', 'ỔI', 'Đ@@', 'Ầ@@', 'U', 'L@@', 'Ị@@', 'CH', 'S@@', 'Ử', 'N@@', 'ƯỚC', 'TA', '<unk>', '<unk>', '<unk>', '(@@', '<unk>', '<unk>', '<unk>', ')@@', 'ʃ@@', '<unk>', '<unk>', '<unk>', '</s>']\n",
      "==========================\n",
      "Ch@@ -> 1735\n",
      "ương@@ -> 50410\n",
      ": -> 27\n",
      "B@@ -> 924\n",
      "U@@ -> 1878\n",
      "ỔI -> 59831\n",
      "Đ@@ -> 1782\n",
      "Ầ@@ -> 28661\n",
      "U -> 3521\n",
      "L@@ -> 1042\n",
      "Ị@@ -> 31584\n",
      "CH -> 6506\n",
      "S@@ -> 870\n",
      "Ử -> 55132\n",
      "N@@ -> 1086\n",
      "ƯỚC -> 55296\n",
      "TA -> 10875\n",
      "❉ -> 3\n",
      "♡@@ -> 3\n",
      "ƪ@@ -> 3\n",
      "(@@ -> 14157\n",
      "ˆ@@ -> 3\n",
      "◡@@ -> 3\n",
      "ˆ@@ -> 3\n",
      ")@@ -> 37272\n",
      "ʃ@@ -> 63120\n",
      "♪ -> 3\n",
      "卍 -> 3\n",
      "₯ -> 3\n"
     ]
    }
   ],
   "source": [
    "# Show inputs\n",
    "for k, v in inputs.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()\n",
    "\n",
    "# Hiển thị chunking\n",
    "print(\"==========================\")\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "\n",
    "# Hiển thị token và mã tương ứng\n",
    "print(\"==========================\")\n",
    "for token, token_id in zip(tokenizer.tokenize(sentence), tokenizer(sentence)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3e2a85cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens length: 368.921498661909\n",
      "Std of tokens length: 313.0481961962608\n",
      "Minimum of tokens on a sample: 0\n",
      "Maximum of tokens on a sample: 1932\n",
      "Subword rate: 0.2056794523661564\n",
      "Out-of-vocab rate: 0.008985373379017848\n"
     ]
    }
   ],
   "source": [
    "all_categories(dataframe=df, tokenizer=tokenizer, subword_character=\"@@\", oov_character=\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b094fc",
   "metadata": {},
   "source": [
    "# sup-SimCSE-VietNamese-phobert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "61f08f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "path = \"../../embedding_models/sup-SimCSE-VietNamese-phobert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fc9dfc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "3\n",
      "{'input_ids': tensor([[    0,  1735, 50410,    27,   924,  1878, 59831,  1782, 28661,  3521,\n",
      "          1042, 31584,  6506,   870, 55132,  1086, 55296, 10875,     3,     3,\n",
      "             3, 14157,     3,     3,     3, 37272, 63120,     3,     3,     3,\n",
      "             2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# Tokenizer cho câu đầu vào\n",
    "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "print(len(inputs))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f1f4e4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 31])\n",
      "tensor([[    0,  1735, 50410,    27,   924,  1878, 59831,  1782, 28661,  3521,\n",
      "          1042, 31584,  6506,   870, 55132,  1086, 55296, 10875,     3,     3,\n",
      "             3, 14157,     3,     3,     3, 37272, 63120,     3,     3,     3,\n",
      "             2]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 31])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 31])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "==========================\n",
      "['<s>', 'Ch@@', 'ương@@', ':', 'B@@', 'U@@', 'ỔI', 'Đ@@', 'Ầ@@', 'U', 'L@@', 'Ị@@', 'CH', 'S@@', 'Ử', 'N@@', 'ƯỚC', 'TA', '<unk>', '<unk>', '<unk>', '(@@', '<unk>', '<unk>', '<unk>', ')@@', 'ʃ@@', '<unk>', '<unk>', '<unk>', '</s>']\n",
      "==========================\n",
      "Ch@@ -> 1735\n",
      "ương@@ -> 50410\n",
      ": -> 27\n",
      "B@@ -> 924\n",
      "U@@ -> 1878\n",
      "ỔI -> 59831\n",
      "Đ@@ -> 1782\n",
      "Ầ@@ -> 28661\n",
      "U -> 3521\n",
      "L@@ -> 1042\n",
      "Ị@@ -> 31584\n",
      "CH -> 6506\n",
      "S@@ -> 870\n",
      "Ử -> 55132\n",
      "N@@ -> 1086\n",
      "ƯỚC -> 55296\n",
      "TA -> 10875\n",
      "❉ -> 3\n",
      "♡@@ -> 3\n",
      "ƪ@@ -> 3\n",
      "(@@ -> 14157\n",
      "ˆ@@ -> 3\n",
      "◡@@ -> 3\n",
      "ˆ@@ -> 3\n",
      ")@@ -> 37272\n",
      "ʃ@@ -> 63120\n",
      "♪ -> 3\n",
      "卍 -> 3\n",
      "₯ -> 3\n"
     ]
    }
   ],
   "source": [
    "# Show inputs\n",
    "for k, v in inputs.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()\n",
    "\n",
    "# Hiển thị chunking\n",
    "print(\"==========================\")\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "\n",
    "# Hiển thị token và mã tương ứng\n",
    "print(\"==========================\")\n",
    "for token, token_id in zip(tokenizer.tokenize(sentence), tokenizer(sentence)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b34db55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens length: 188.19446922390722\n",
      "Std of tokens length: 86.92475036676232\n",
      "Minimum of tokens on a sample: 0\n",
      "Maximum of tokens on a sample: 256\n",
      "Subword rate: 0.22599850212830502\n",
      "Out-of-vocab rate: 0.00996843093199852\n"
     ]
    }
   ],
   "source": [
    "all_categories(dataframe=df, tokenizer=tokenizer, subword_character=\"@@\", oov_character=\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3e268",
   "metadata": {},
   "source": [
    "# vietnamese-bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a621b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "path = \"../../embedding_models/vietnamese-bi-encoder\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "506bd2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000000000019884624838656\n",
      "3\n",
      "{'input_ids': tensor([[    0,  1735, 50410,    27,   924,  1878, 59831,  1782, 28661,  3521,\n",
      "          1042, 31584,  6506,   870, 55132,  1086, 55296, 10875,     3,     3,\n",
      "             3, 14157,     3,     3,     3, 37272, 63120,     3,     3,     3,\n",
      "             2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# Tokenizer cho câu đầu vào\n",
    "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "print(len(inputs))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "289ee4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 31])\n",
      "tensor([[    0,  1735, 50410,    27,   924,  1878, 59831,  1782, 28661,  3521,\n",
      "          1042, 31584,  6506,   870, 55132,  1086, 55296, 10875,     3,     3,\n",
      "             3, 14157,     3,     3,     3, 37272, 63120,     3,     3,     3,\n",
      "             2]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 31])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 31])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "==========================\n",
      "['<s>', 'Ch@@', 'ương@@', ':', 'B@@', 'U@@', 'ỔI', 'Đ@@', 'Ầ@@', 'U', 'L@@', 'Ị@@', 'CH', 'S@@', 'Ử', 'N@@', 'ƯỚC', 'TA', '<unk>', '<unk>', '<unk>', '(@@', '<unk>', '<unk>', '<unk>', ')@@', 'ʃ@@', '<unk>', '<unk>', '<unk>', '</s>']\n",
      "==========================\n",
      "Ch@@ -> 1735\n",
      "ương@@ -> 50410\n",
      ": -> 27\n",
      "B@@ -> 924\n",
      "U@@ -> 1878\n",
      "ỔI -> 59831\n",
      "Đ@@ -> 1782\n",
      "Ầ@@ -> 28661\n",
      "U -> 3521\n",
      "L@@ -> 1042\n",
      "Ị@@ -> 31584\n",
      "CH -> 6506\n",
      "S@@ -> 870\n",
      "Ử -> 55132\n",
      "N@@ -> 1086\n",
      "ƯỚC -> 55296\n",
      "TA -> 10875\n",
      "❉ -> 3\n",
      "♡@@ -> 3\n",
      "ƪ@@ -> 3\n",
      "(@@ -> 14157\n",
      "ˆ@@ -> 3\n",
      "◡@@ -> 3\n",
      "ˆ@@ -> 3\n",
      ")@@ -> 37272\n",
      "ʃ@@ -> 63120\n",
      "♪ -> 3\n",
      "卍 -> 3\n",
      "₯ -> 3\n"
     ]
    }
   ],
   "source": [
    "# Show inputs\n",
    "for k, v in inputs.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()\n",
    "\n",
    "# Hiển thị chunking\n",
    "print(\"==========================\")\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "\n",
    "# Hiển thị token và mã tương ứng\n",
    "print(\"==========================\")\n",
    "for token, token_id in zip(tokenizer.tokenize(sentence), tokenizer(sentence)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0cdde963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens length: 368.921498661909\n",
      "Std of tokens length: 313.0481961962608\n",
      "Minimum of tokens on a sample: 0\n",
      "Maximum of tokens on a sample: 1932\n",
      "Subword rate: 0.2056794523661564\n",
      "Out-of-vocab rate: 0.008985373379017848\n"
     ]
    }
   ],
   "source": [
    "all_categories(dataframe=df, tokenizer=tokenizer, subword_character=\"@@\", oov_character=\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e89350",
   "metadata": {},
   "source": [
    "# vietnamese-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "51f66027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "path = \"../../embedding_models/vietnamese-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2b70692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "3\n",
      "{'input_ids': tensor([[    0,  1735, 50410,    27,   924,  1878, 59831,  1782, 28661,  3521,\n",
      "          1042, 31584,  6506,   870, 55132,  1086, 55296, 10875,     3,     3,\n",
      "             3, 14157,     3,     3,     3, 37272, 63120,     3,     3,     3,\n",
      "             2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# Tokenizer cho câu đầu vào\n",
    "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "print(len(inputs))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4b0189e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 31])\n",
      "tensor([[    0,  1735, 50410,    27,   924,  1878, 59831,  1782, 28661,  3521,\n",
      "          1042, 31584,  6506,   870, 55132,  1086, 55296, 10875,     3,     3,\n",
      "             3, 14157,     3,     3,     3, 37272, 63120,     3,     3,     3,\n",
      "             2]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 31])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 31])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "==========================\n",
      "['<s>', 'Ch@@', 'ương@@', ':', 'B@@', 'U@@', 'ỔI', 'Đ@@', 'Ầ@@', 'U', 'L@@', 'Ị@@', 'CH', 'S@@', 'Ử', 'N@@', 'ƯỚC', 'TA', '<unk>', '<unk>', '<unk>', '(@@', '<unk>', '<unk>', '<unk>', ')@@', 'ʃ@@', '<unk>', '<unk>', '<unk>', '</s>']\n",
      "==========================\n",
      "Ch@@ -> 1735\n",
      "ương@@ -> 50410\n",
      ": -> 27\n",
      "B@@ -> 924\n",
      "U@@ -> 1878\n",
      "ỔI -> 59831\n",
      "Đ@@ -> 1782\n",
      "Ầ@@ -> 28661\n",
      "U -> 3521\n",
      "L@@ -> 1042\n",
      "Ị@@ -> 31584\n",
      "CH -> 6506\n",
      "S@@ -> 870\n",
      "Ử -> 55132\n",
      "N@@ -> 1086\n",
      "ƯỚC -> 55296\n",
      "TA -> 10875\n",
      "❉ -> 3\n",
      "♡@@ -> 3\n",
      "ƪ@@ -> 3\n",
      "(@@ -> 14157\n",
      "ˆ@@ -> 3\n",
      "◡@@ -> 3\n",
      "ˆ@@ -> 3\n",
      ")@@ -> 37272\n",
      "ʃ@@ -> 63120\n",
      "♪ -> 3\n",
      "卍 -> 3\n",
      "₯ -> 3\n"
     ]
    }
   ],
   "source": [
    "# Show inputs\n",
    "for k, v in inputs.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()\n",
    "\n",
    "# Hiển thị chunking\n",
    "print(\"==========================\")\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "\n",
    "# Hiển thị token và mã tương ứng\n",
    "print(\"==========================\")\n",
    "for token, token_id in zip(tokenizer.tokenize(sentence), tokenizer(sentence)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ba284015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens length: 293.11329170383584\n",
      "Std of tokens length: 185.41635433026227\n",
      "Minimum of tokens on a sample: 0\n",
      "Maximum of tokens on a sample: 512\n",
      "Subword rate: 0.21116318704729442\n",
      "Out-of-vocab rate: 0.00945888368129527\n"
     ]
    }
   ],
   "source": [
    "all_categories(dataframe=df, tokenizer=tokenizer, subword_character=\"@@\", oov_character=\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb4594",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770da2ec",
   "metadata": {},
   "source": [
    "Để so sánh **hiệu quả tokenization** giữa các tokenizer/model khác nhau, có thể dùng một số **chỉ số định lượng và định tính** :\n",
    "\n",
    "\n",
    "### 🧮 **1. Chỉ số định lượng (Quantitative Metrics)**\n",
    "\n",
    "#### ✅ **a. Trung bình số tokens / mẫu**\n",
    "\n",
    "* **Mục đích**: Đánh giá độ “nén” của tokenizer (ít tokens hơn cho cùng nội dung → tốt hơn).\n",
    "* **Công thức**:\n",
    "\n",
    "  $$\n",
    "  \\text{Avg. tokens per sample} = \\frac{\\sum_{i=1}^{N} \\text{len(tokens}_i\\text{)}}{N}\n",
    "  $$\n",
    "\n",
    "#### ✅ **b. Tỷ lệ “subword splitting”**\n",
    "\n",
    "* **Mục đích**: Tokenizer tốt sẽ ít phải cắt từ tiếng Việt thành nhiều mảnh.\n",
    "* **Công thức**:\n",
    "\n",
    "  $$\n",
    "  \\text{Subword ratio} = \\frac{\\text{Tổng số subwords}}{\\text{Tổng số từ gốc}}\n",
    "  $$\n",
    "\n",
    "#### ✅ **c. Vocabulary coverage (nếu có từ điển chuẩn)**\n",
    "\n",
    "* **Mục đích**: Đo mức độ bao phủ từ vựng tiếng Việt thực tế.\n",
    "* So với từ điển chuẩn hoặc tập wordlist corpus.\n",
    "* **Chỉ số**:\n",
    "\n",
    "  * % từ vựng có trong vocab\n",
    "  * % từ vựng bị tách thành subwords\n",
    "\n",
    "#### ✅ **d. Tỷ lệ token đặc biệt / padding**\n",
    "\n",
    "* Xem tokenizer có sinh nhiều `[PAD]`, `[UNK]` không (dấu hiệu thiếu hiệu quả).\n",
    "\n",
    "\n",
    "### 📊 **2. Chỉ số định tính (Qualitative Analysis)**\n",
    "\n",
    "#### 🔍 **a. Các ví dụ token hóa thực tế**\n",
    "\n",
    "* So sánh cách tokenizer xử lý các câu tiếng Việt có:\n",
    "\n",
    "  * từ ghép\n",
    "  * dấu tiếng Việt (dấu hỏi, ngã, sắc…)\n",
    "  * từ mượn, từ chuyên ngành\n",
    "  * câu ngắn/dài, chính tả không chuẩn\n",
    "\n",
    "#### 🎯 **b. Độ nguyên vẹn từ vựng**\n",
    "\n",
    "* Tokenizer tốt nên giữ được từ đơn/từ ghép tiếng Việt như `\"học sinh\"`, `\"giáo viên\"` thay vì `\"học\", \"##sinh\"`\n",
    "\n",
    "\n",
    "### ⚙️ **3. Thực nghiệm phụ trợ (nếu cần mở rộng)**\n",
    "\n",
    "* Thử huấn luyện nhanh 1 downstream task như classification (sentiment, intent, etc.) với từng tokenizer → so sánh F1/accuracy để xem ảnh hưởng thực tế.\n",
    "* Đo thời gian tokenize + memory usage nếu đánh giá hiệu suất.\n",
    "\n",
    "\n",
    "### 📑 Gợi ý trình bày báo cáo\n",
    "\n",
    "| Mô hình   | Avg Tokens | Subword Ratio | % OOV | Ví dụ token hóa                        |\n",
    "| --------- | ---------- | ------------- | ----- | -------------------------------------- |\n",
    "| BERT Pho  | 54.3       | 1.21          | 2.3%  | \"giáo viên\" → `['giáo', 'viên']`       |\n",
    "| GPT2-ViT5 | 48.6       | 1.09          | 1.1%  | \"giáo viên\" → `['giáo viên']`          |\n",
    "| mBERT     | 61.2       | 1.35          | 3.7%  | \"giáo viên\" → `['gi', '##áo', 'viên']` |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
