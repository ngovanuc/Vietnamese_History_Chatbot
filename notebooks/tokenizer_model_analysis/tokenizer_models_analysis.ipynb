{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23cd9b9",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ea4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1227db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUỔI ĐẦU LỊCH SỬ NƯỚC TA</td>\n",
       "      <td>Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA</td>\n",
       "      <td>chapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA</td>\n",
       "      <td>Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA\\nBài: THỜI NG...</td>\n",
       "      <td>lesson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Những dấu tích của Người tối cổ được tìm thấy ...</td>\n",
       "      <td>Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nNhững ...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ở giai đoạn đầu, Người tinh khôn sống như thế ...</td>\n",
       "      <td>Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nỞ giai...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giai đoạn phát triển của Người tinh khôn có gì...</td>\n",
       "      <td>Bài: THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA\\nGiai đ...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  ...     type\n",
       "0                           BUỔI ĐẦU LỊCH SỬ NƯỚC TA  ...  chapter\n",
       "1                  THỜI NGUYÊN THUỶ TRÊN ĐẤT NƯỚC TA  ...   lesson\n",
       "2  Những dấu tích của Người tối cổ được tìm thấy ...  ...    title\n",
       "3  Ở giai đoạn đầu, Người tinh khôn sống như thế ...  ...    title\n",
       "4  Giai đoạn phát triển của Người tinh khôn có gì...  ...    title\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vietnamese_history_dataset_path = \"../../datasets/vietnamese_history_dataset/data.json\"\n",
    "df = pd.read_json(vietnamese_history_dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447c238",
   "metadata": {},
   "source": [
    "# gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2624428b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13bd15dddad4dfe93786468cb08939c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff59df3d72c4656ac87472171eadc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37baeafcdc542df895afbf6c523fb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d548583728da4265a124d57d1f228a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1b3142b85349df9af427b10be1c2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", cache_dir=\"../../tokenizer_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f5aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[0][\"content\"]\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed95bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 35])\n",
      "tensor([[ 1925,   130,   108,   130,    94,   782,    25, 20571,   157,   119,\n",
      "           242,    40, 34754,   238,   157,   118,    99,    52,   406,   157,\n",
      "           119,   232,  3398,   311,   157,   119,   105,   399,   130,   107,\n",
      "           157,   119,   248,    34, 21664]])\n",
      "\n",
      "attention_mask: torch.Size([1, 35])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokens.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e558755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch', 'Æ', '°', 'Æ', '¡', 'ng', ':', 'ĠBU', 'á', '»', 'Ķ', 'I', 'ĠÄ', 'Ĳ', 'á', 'º', '¦', 'U', 'ĠL', 'á', '»', 'Ĭ', 'CH', 'ĠS', 'á', '»', '¬', 'ĠN', 'Æ', '¯', 'á', '»', 'ļ', 'C', 'ĠTA']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2792d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch -> 130\n",
      "Æ -> 108\n",
      "° -> 130\n",
      "Æ -> 94\n",
      "¡ -> 782\n",
      "ng -> 25\n",
      ": -> 20571\n",
      "ĠBU -> 157\n",
      "á -> 119\n",
      "» -> 242\n",
      "Ķ -> 40\n",
      "I -> 34754\n",
      "ĠÄ -> 238\n",
      "Ĳ -> 157\n",
      "á -> 118\n",
      "º -> 99\n",
      "¦ -> 52\n",
      "U -> 406\n",
      "ĠL -> 157\n",
      "á -> 119\n",
      "» -> 232\n",
      "Ĭ -> 3398\n",
      "CH -> 311\n",
      "ĠS -> 157\n",
      "á -> 119\n",
      "» -> 105\n",
      "¬ -> 399\n",
      "ĠN -> 130\n",
      "Æ -> 107\n",
      "¯ -> 157\n",
      "á -> 119\n",
      "» -> 248\n",
      "ļ -> 34\n"
     ]
    }
   ],
   "source": [
    "for token, token_id in zip(tokenizer.tokenize(text), tokenizer(text)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a106bd",
   "metadata": {},
   "source": [
    "# NlpHUST/ner-vietnamese-electra-base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54edc535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e4dbbb2d9c4bbab45e2e9a7a2fe73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e50dc487b4c4907b27cf14949413ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/411k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7993922a0ed84202b176051fd06bc933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2615c2de0f745f192bb1005eeb1427e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"NlpHUST/ner-vietnamese-electra-base\",\n",
    "    cache_dir=\"../../tokenizer_models/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9606c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA\n"
     ]
    }
   ],
   "source": [
    "text = df.iloc[0][\"content\"]\n",
    "print(text)\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6e7bc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 10])\n",
      "tensor([[    2,  1110,    42,     1,     1,     1,     1,     1, 15462,     3]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 10])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 10])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokens.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d2dd789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Chương', ':', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'TA', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "617f03ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chương -> 1110\n",
      ": -> 42\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "TA -> 15462\n"
     ]
    }
   ],
   "source": [
    "for token, token_id in zip(tokenizer.tokenize(text), tokenizer(text)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b1bfe",
   "metadata": {},
   "source": [
    "# trituenhantaoio/bert-base-vietnamese-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b3b8a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"trituenhantaoio/bert-base-vietnamese-uncased\",\n",
    "    cache_dir=\"../../tokenizer_models/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb547dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA\n"
     ]
    }
   ],
   "source": [
    "text = df.iloc[0][\"content\"]\n",
    "print(text)\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa3264d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 13])\n",
      "tensor([[    2, 22360,     1,  3710,   374,    27,   881, 15780,  1460,  3086,\n",
      "          3332,   389,     3]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 13])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 13])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokens.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c42974ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'chuong', '[UNK]', 'bu', '##o', '##i', 'đau', 'lich', 'su', 'nu', '##oc', 'ta', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b832c5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chuong -> 22360\n",
      "[UNK] -> 1\n",
      "bu -> 3710\n",
      "##o -> 374\n",
      "##i -> 27\n",
      "đau -> 881\n",
      "lich -> 15780\n",
      "su -> 1460\n",
      "nu -> 3086\n",
      "##oc -> 3332\n",
      "ta -> 389\n"
     ]
    }
   ],
   "source": [
    "for token, token_id in zip(tokenizer.tokenize(text), tokenizer(text)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bada548",
   "metadata": {},
   "source": [
    "# Babelscape/wikineural-multilingual-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89f438b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcba9dfc89454ca0a1675234aee03232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b023f7a43fd4f70b559f4410984c666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea6ec14d4a34eb5acc3c58ee69ad465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0541ca9d3c9847b59df78bf800b24320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Babelscape/wikineural-multilingual-ner\",\n",
    "    cache_dir=\"../../tokenizer_models/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ea16711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chương: BUỔI ĐẦU LỊCH SỬ NƯỚC TA\n"
     ]
    }
   ],
   "source": [
    "text = df.iloc[0][\"content\"]\n",
    "print(text)\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33839d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 10])\n",
      "tensor([[  101, 88797,   131,   100,   100,   100,   100,   100, 91075,   102]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 10])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 10])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokens.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e3c600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Chương', ':', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'TA', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75ab926b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chương -> 88797\n",
      ": -> 131\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "TA -> 91075\n"
     ]
    }
   ],
   "source": [
    "for token, token_id in zip(tokenizer.tokenize(text), tokenizer(text)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb4594",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770da2ec",
   "metadata": {},
   "source": [
    "Để so sánh **hiệu quả tokenization** giữa các tokenizer/model khác nhau, có thể dùng một số **chỉ số định lượng và định tính** :\n",
    "\n",
    "\n",
    "### 🧮 **1. Chỉ số định lượng (Quantitative Metrics)**\n",
    "\n",
    "#### ✅ **a. Trung bình số tokens / mẫu**\n",
    "\n",
    "* **Mục đích**: Đánh giá độ “nén” của tokenizer (ít tokens hơn cho cùng nội dung → tốt hơn).\n",
    "* **Công thức**:\n",
    "\n",
    "  $$\n",
    "  \\text{Avg. tokens per sample} = \\frac{\\sum_{i=1}^{N} \\text{len(tokens}_i\\text{)}}{N}\n",
    "  $$\n",
    "\n",
    "#### ✅ **b. Tỷ lệ “subword splitting”**\n",
    "\n",
    "* **Mục đích**: Tokenizer tốt sẽ ít phải cắt từ tiếng Việt thành nhiều mảnh.\n",
    "* **Công thức**:\n",
    "\n",
    "  $$\n",
    "  \\text{Subword ratio} = \\frac{\\text{Tổng số subwords}}{\\text{Tổng số từ gốc}}\n",
    "  $$\n",
    "\n",
    "#### ✅ **c. Vocabulary coverage (nếu có từ điển chuẩn)**\n",
    "\n",
    "* **Mục đích**: Đo mức độ bao phủ từ vựng tiếng Việt thực tế.\n",
    "* So với từ điển chuẩn hoặc tập wordlist corpus.\n",
    "* **Chỉ số**:\n",
    "\n",
    "  * % từ vựng có trong vocab\n",
    "  * % từ vựng bị tách thành subwords\n",
    "\n",
    "#### ✅ **d. Tỷ lệ token đặc biệt / padding**\n",
    "\n",
    "* Xem tokenizer có sinh nhiều `[PAD]`, `[UNK]` không (dấu hiệu thiếu hiệu quả).\n",
    "\n",
    "\n",
    "### 📊 **2. Chỉ số định tính (Qualitative Analysis)**\n",
    "\n",
    "#### 🔍 **a. Các ví dụ token hóa thực tế**\n",
    "\n",
    "* So sánh cách tokenizer xử lý các câu tiếng Việt có:\n",
    "\n",
    "  * từ ghép\n",
    "  * dấu tiếng Việt (dấu hỏi, ngã, sắc…)\n",
    "  * từ mượn, từ chuyên ngành\n",
    "  * câu ngắn/dài, chính tả không chuẩn\n",
    "\n",
    "#### 🎯 **b. Độ nguyên vẹn từ vựng**\n",
    "\n",
    "* Tokenizer tốt nên giữ được từ đơn/từ ghép tiếng Việt như `\"học sinh\"`, `\"giáo viên\"` thay vì `\"học\", \"##sinh\"`\n",
    "\n",
    "\n",
    "### ⚙️ **3. Thực nghiệm phụ trợ (nếu cần mở rộng)**\n",
    "\n",
    "* Thử huấn luyện nhanh 1 downstream task như classification (sentiment, intent, etc.) với từng tokenizer → so sánh F1/accuracy để xem ảnh hưởng thực tế.\n",
    "* Đo thời gian tokenize + memory usage nếu đánh giá hiệu suất.\n",
    "\n",
    "\n",
    "### 📑 Gợi ý trình bày báo cáo\n",
    "\n",
    "| Mô hình   | Avg Tokens | Subword Ratio | % OOV | Ví dụ token hóa                        |\n",
    "| --------- | ---------- | ------------- | ----- | -------------------------------------- |\n",
    "| BERT Pho  | 54.3       | 1.21          | 2.3%  | \"giáo viên\" → `['giáo', 'viên']`       |\n",
    "| GPT2-ViT5 | 48.6       | 1.09          | 1.1%  | \"giáo viên\" → `['giáo viên']`          |\n",
    "| mBERT     | 61.2       | 1.35          | 3.7%  | \"giáo viên\" → `['gi', '##áo', 'viên']` |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
