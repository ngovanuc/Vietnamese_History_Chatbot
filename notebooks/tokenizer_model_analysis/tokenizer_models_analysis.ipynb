{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23cd9b9",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ea4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1227db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BU·ªîI ƒê·∫¶U L·ªäCH S·ª¨ N∆Ø·ªöC TA</td>\n",
       "      <td>Ch∆∞∆°ng: BU·ªîI ƒê·∫¶U L·ªäCH S·ª¨ N∆Ø·ªöC TA</td>\n",
       "      <td>chapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TH·ªúI NGUY√äN THU·ª∂ TR√äN ƒê·∫§T N∆Ø·ªöC TA</td>\n",
       "      <td>Ch∆∞∆°ng: BU·ªîI ƒê·∫¶U L·ªäCH S·ª¨ N∆Ø·ªöC TA\\nB√†i: TH·ªúI NG...</td>\n",
       "      <td>lesson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nh·ªØng d·∫•u t√≠ch c·ªßa Ng∆∞·ªùi t·ªëi c·ªï ƒë∆∞·ª£c t√¨m th·∫•y ...</td>\n",
       "      <td>B√†i: TH·ªúI NGUY√äN THU·ª∂ TR√äN ƒê·∫§T N∆Ø·ªöC TA\\nNh·ªØng ...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>·ªû giai ƒëo·∫°n ƒë·∫ßu, Ng∆∞·ªùi tinh kh√¥n s·ªëng nh∆∞ th·∫ø ...</td>\n",
       "      <td>B√†i: TH·ªúI NGUY√äN THU·ª∂ TR√äN ƒê·∫§T N∆Ø·ªöC TA\\n·ªû giai...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giai ƒëo·∫°n ph√°t tri·ªÉn c·ªßa Ng∆∞·ªùi tinh kh√¥n c√≥ g√¨...</td>\n",
       "      <td>B√†i: TH·ªúI NGUY√äN THU·ª∂ TR√äN ƒê·∫§T N∆Ø·ªöC TA\\nGiai ƒë...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  ...     type\n",
       "0                           BU·ªîI ƒê·∫¶U L·ªäCH S·ª¨ N∆Ø·ªöC TA  ...  chapter\n",
       "1                  TH·ªúI NGUY√äN THU·ª∂ TR√äN ƒê·∫§T N∆Ø·ªöC TA  ...   lesson\n",
       "2  Nh·ªØng d·∫•u t√≠ch c·ªßa Ng∆∞·ªùi t·ªëi c·ªï ƒë∆∞·ª£c t√¨m th·∫•y ...  ...    title\n",
       "3  ·ªû giai ƒëo·∫°n ƒë·∫ßu, Ng∆∞·ªùi tinh kh√¥n s·ªëng nh∆∞ th·∫ø ...  ...    title\n",
       "4  Giai ƒëo·∫°n ph√°t tri·ªÉn c·ªßa Ng∆∞·ªùi tinh kh√¥n c√≥ g√¨...  ...    title\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vietnamese_history_dataset_path = \"../../datasets/vietnamese_history_dataset/data.json\"\n",
    "df = pd.read_json(vietnamese_history_dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447c238",
   "metadata": {},
   "source": [
    "# gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2624428b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13bd15dddad4dfe93786468cb08939c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff59df3d72c4656ac87472171eadc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37baeafcdc542df895afbf6c523fb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d548583728da4265a124d57d1f228a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1b3142b85349df9af427b10be1c2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", cache_dir=\"../../tokenizer_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f5aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[0][\"content\"]\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed95bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 35])\n",
      "tensor([[ 1925,   130,   108,   130,    94,   782,    25, 20571,   157,   119,\n",
      "           242,    40, 34754,   238,   157,   118,    99,    52,   406,   157,\n",
      "           119,   232,  3398,   311,   157,   119,   105,   399,   130,   107,\n",
      "           157,   119,   248,    34, 21664]])\n",
      "\n",
      "attention_mask: torch.Size([1, 35])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokens.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e558755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch', '√Ü', '¬∞', '√Ü', '¬°', 'ng', ':', 'ƒ†BU', '√°', '¬ª', 'ƒ∂', 'I', 'ƒ†√Ñ', 'ƒ≤', '√°', '¬∫', '¬¶', 'U', 'ƒ†L', '√°', '¬ª', 'ƒ¨', 'CH', 'ƒ†S', '√°', '¬ª', '¬¨', 'ƒ†N', '√Ü', '¬Ø', '√°', '¬ª', 'ƒº', 'C', 'ƒ†TA']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2792d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch -> 130\n",
      "√Ü -> 108\n",
      "¬∞ -> 130\n",
      "√Ü -> 94\n",
      "¬° -> 782\n",
      "ng -> 25\n",
      ": -> 20571\n",
      "ƒ†BU -> 157\n",
      "√° -> 119\n",
      "¬ª -> 242\n",
      "ƒ∂ -> 40\n",
      "I -> 34754\n",
      "ƒ†√Ñ -> 238\n",
      "ƒ≤ -> 157\n",
      "√° -> 118\n",
      "¬∫ -> 99\n",
      "¬¶ -> 52\n",
      "U -> 406\n",
      "ƒ†L -> 157\n",
      "√° -> 119\n",
      "¬ª -> 232\n",
      "ƒ¨ -> 3398\n",
      "CH -> 311\n",
      "ƒ†S -> 157\n",
      "√° -> 119\n",
      "¬ª -> 105\n",
      "¬¨ -> 399\n",
      "ƒ†N -> 130\n",
      "√Ü -> 107\n",
      "¬Ø -> 157\n",
      "√° -> 119\n",
      "¬ª -> 248\n",
      "ƒº -> 34\n"
     ]
    }
   ],
   "source": [
    "for token, token_id in zip(tokenizer.tokenize(text), tokenizer(text)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a106bd",
   "metadata": {},
   "source": [
    "# NlpHUST/ner-vietnamese-electra-base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54edc535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e4dbbb2d9c4bbab45e2e9a7a2fe73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e50dc487b4c4907b27cf14949413ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/411k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7993922a0ed84202b176051fd06bc933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2615c2de0f745f192bb1005eeb1427e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"NlpHUST/ner-vietnamese-electra-base\",\n",
    "    cache_dir=\"../../tokenizer_models/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9606c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch∆∞∆°ng: BU·ªîI ƒê·∫¶U L·ªäCH S·ª¨ N∆Ø·ªöC TA\n"
     ]
    }
   ],
   "source": [
    "text = df.iloc[0][\"content\"]\n",
    "print(text)\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6e7bc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 10])\n",
      "tensor([[    2,  1110,    42,     1,     1,     1,     1,     1, 15462,     3]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 10])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 10])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokens.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d2dd789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Ch∆∞∆°ng', ':', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'TA', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "617f03ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch∆∞∆°ng -> 1110\n",
      ": -> 42\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "[UNK] -> 1\n",
      "TA -> 15462\n"
     ]
    }
   ],
   "source": [
    "for token, token_id in zip(tokenizer.tokenize(text), tokenizer(text)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b1bfe",
   "metadata": {},
   "source": [
    "# trituenhantaoio/bert-base-vietnamese-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b3b8a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"trituenhantaoio/bert-base-vietnamese-uncased\",\n",
    "    cache_dir=\"../../tokenizer_models/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb547dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch∆∞∆°ng: BU·ªîI ƒê·∫¶U L·ªäCH S·ª¨ N∆Ø·ªöC TA\n"
     ]
    }
   ],
   "source": [
    "text = df.iloc[0][\"content\"]\n",
    "print(text)\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa3264d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 13])\n",
      "tensor([[    2, 22360,     1,  3710,   374,    27,   881, 15780,  1460,  3086,\n",
      "          3332,   389,     3]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 13])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 13])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokens.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c42974ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'chuong', '[UNK]', 'bu', '##o', '##i', 'ƒëau', 'lich', 'su', 'nu', '##oc', 'ta', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b832c5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chuong -> 22360\n",
      "[UNK] -> 1\n",
      "bu -> 3710\n",
      "##o -> 374\n",
      "##i -> 27\n",
      "ƒëau -> 881\n",
      "lich -> 15780\n",
      "su -> 1460\n",
      "nu -> 3086\n",
      "##oc -> 3332\n",
      "ta -> 389\n"
     ]
    }
   ],
   "source": [
    "for token, token_id in zip(tokenizer.tokenize(text), tokenizer(text)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bada548",
   "metadata": {},
   "source": [
    "# Babelscape/wikineural-multilingual-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89f438b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcba9dfc89454ca0a1675234aee03232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b023f7a43fd4f70b559f4410984c666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea6ec14d4a34eb5acc3c58ee69ad465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0541ca9d3c9847b59df78bf800b24320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Babelscape/wikineural-multilingual-ner\",\n",
    "    cache_dir=\"../../tokenizer_models/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ea16711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch∆∞∆°ng: BU·ªîI ƒê·∫¶U L·ªäCH S·ª¨ N∆Ø·ªöC TA\n"
     ]
    }
   ],
   "source": [
    "text = df.iloc[0][\"content\"]\n",
    "print(text)\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33839d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 10])\n",
      "tensor([[  101, 88797,   131,   100,   100,   100,   100,   100, 91075,   102]])\n",
      "\n",
      "token_type_ids: torch.Size([1, 10])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "attention_mask: torch.Size([1, 10])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokens.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e3c600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Ch∆∞∆°ng', ':', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'TA', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75ab926b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch∆∞∆°ng -> 88797\n",
      ": -> 131\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "[UNK] -> 100\n",
      "TA -> 91075\n"
     ]
    }
   ],
   "source": [
    "for token, token_id in zip(tokenizer.tokenize(text), tokenizer(text)[\"input_ids\"][1:-1]):\n",
    "    print(f\"{token} -> {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb4594",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770da2ec",
   "metadata": {},
   "source": [
    "ƒê·ªÉ so s√°nh **hi·ªáu qu·∫£ tokenization** gi·ªØa c√°c tokenizer/model kh√°c nhau, c√≥ th·ªÉ d√πng m·ªôt s·ªë **ch·ªâ s·ªë ƒë·ªãnh l∆∞·ª£ng v√† ƒë·ªãnh t√≠nh** :\n",
    "\n",
    "\n",
    "### üßÆ **1. Ch·ªâ s·ªë ƒë·ªãnh l∆∞·ª£ng (Quantitative Metrics)**\n",
    "\n",
    "#### ‚úÖ **a. Trung b√¨nh s·ªë tokens / m·∫´u**\n",
    "\n",
    "* **M·ª•c ƒë√≠ch**: ƒê√°nh gi√° ƒë·ªô ‚Äún√©n‚Äù c·ªßa tokenizer (√≠t tokens h∆°n cho c√πng n·ªôi dung ‚Üí t·ªët h∆°n).\n",
    "* **C√¥ng th·ª©c**:\n",
    "\n",
    "  $$\n",
    "  \\text{Avg. tokens per sample} = \\frac{\\sum_{i=1}^{N} \\text{len(tokens}_i\\text{)}}{N}\n",
    "  $$\n",
    "\n",
    "#### ‚úÖ **b. T·ª∑ l·ªá ‚Äúsubword splitting‚Äù**\n",
    "\n",
    "* **M·ª•c ƒë√≠ch**: Tokenizer t·ªët s·∫Ω √≠t ph·∫£i c·∫Øt t·ª´ ti·∫øng Vi·ªát th√†nh nhi·ªÅu m·∫£nh.\n",
    "* **C√¥ng th·ª©c**:\n",
    "\n",
    "  $$\n",
    "  \\text{Subword ratio} = \\frac{\\text{T·ªïng s·ªë subwords}}{\\text{T·ªïng s·ªë t·ª´ g·ªëc}}\n",
    "  $$\n",
    "\n",
    "#### ‚úÖ **c. Vocabulary coverage (n·∫øu c√≥ t·ª´ ƒëi·ªÉn chu·∫©n)**\n",
    "\n",
    "* **M·ª•c ƒë√≠ch**: ƒêo m·ª©c ƒë·ªô bao ph·ªß t·ª´ v·ª±ng ti·∫øng Vi·ªát th·ª±c t·∫ø.\n",
    "* So v·ªõi t·ª´ ƒëi·ªÉn chu·∫©n ho·∫∑c t·∫≠p wordlist corpus.\n",
    "* **Ch·ªâ s·ªë**:\n",
    "\n",
    "  * % t·ª´ v·ª±ng c√≥ trong vocab\n",
    "  * % t·ª´ v·ª±ng b·ªã t√°ch th√†nh subwords\n",
    "\n",
    "#### ‚úÖ **d. T·ª∑ l·ªá token ƒë·∫∑c bi·ªát / padding**\n",
    "\n",
    "* Xem tokenizer c√≥ sinh nhi·ªÅu `[PAD]`, `[UNK]` kh√¥ng (d·∫•u hi·ªáu thi·∫øu hi·ªáu qu·∫£).\n",
    "\n",
    "\n",
    "### üìä **2. Ch·ªâ s·ªë ƒë·ªãnh t√≠nh (Qualitative Analysis)**\n",
    "\n",
    "#### üîç **a. C√°c v√≠ d·ª• token h√≥a th·ª±c t·∫ø**\n",
    "\n",
    "* So s√°nh c√°ch tokenizer x·ª≠ l√Ω c√°c c√¢u ti·∫øng Vi·ªát c√≥:\n",
    "\n",
    "  * t·ª´ gh√©p\n",
    "  * d·∫•u ti·∫øng Vi·ªát (d·∫•u h·ªèi, ng√£, s·∫Øc‚Ä¶)\n",
    "  * t·ª´ m∆∞·ª£n, t·ª´ chuy√™n ng√†nh\n",
    "  * c√¢u ng·∫Øn/d√†i, ch√≠nh t·∫£ kh√¥ng chu·∫©n\n",
    "\n",
    "#### üéØ **b. ƒê·ªô nguy√™n v·∫πn t·ª´ v·ª±ng**\n",
    "\n",
    "* Tokenizer t·ªët n√™n gi·ªØ ƒë∆∞·ª£c t·ª´ ƒë∆°n/t·ª´ gh√©p ti·∫øng Vi·ªát nh∆∞ `\"h·ªçc sinh\"`, `\"gi√°o vi√™n\"` thay v√¨ `\"h·ªçc\", \"##sinh\"`\n",
    "\n",
    "\n",
    "### ‚öôÔ∏è **3. Th·ª±c nghi·ªám ph·ª• tr·ª£ (n·∫øu c·∫ßn m·ªü r·ªông)**\n",
    "\n",
    "* Th·ª≠ hu·∫•n luy·ªán nhanh 1 downstream task nh∆∞ classification (sentiment, intent, etc.) v·ªõi t·ª´ng tokenizer ‚Üí so s√°nh F1/accuracy ƒë·ªÉ xem ·∫£nh h∆∞·ªüng th·ª±c t·∫ø.\n",
    "* ƒêo th·ªùi gian tokenize + memory usage n·∫øu ƒë√°nh gi√° hi·ªáu su·∫•t.\n",
    "\n",
    "\n",
    "### üìë G·ª£i √Ω tr√¨nh b√†y b√°o c√°o\n",
    "\n",
    "| M√¥ h√¨nh   | Avg Tokens | Subword Ratio | % OOV | V√≠ d·ª• token h√≥a                        |\n",
    "| --------- | ---------- | ------------- | ----- | -------------------------------------- |\n",
    "| BERT Pho  | 54.3       | 1.21          | 2.3%  | \"gi√°o vi√™n\" ‚Üí `['gi√°o', 'vi√™n']`       |\n",
    "| GPT2-ViT5 | 48.6       | 1.09          | 1.1%  | \"gi√°o vi√™n\" ‚Üí `['gi√°o vi√™n']`          |\n",
    "| mBERT     | 61.2       | 1.35          | 3.7%  | \"gi√°o vi√™n\" ‚Üí `['gi', '##√°o', 'vi√™n']` |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
